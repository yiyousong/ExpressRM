{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8ab9a3-c72f-477a-a6ed-e0effa96e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from torch import nn\n",
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from functools import reduce\n",
    "from subprocess import Popen\n",
    "from Bio import SeqIO\n",
    "header='/home/yiyou/test/tmpout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f0720fd-9aa1-48bc-bea4-47159a76aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeplift\n",
      "  Downloading deeplift-0.6.13.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: numpy>=1.9 in /home/yiyou/miniconda3/lib/python3.8/site-packages (from deeplift) (1.24.4)\n",
      "Building wheels for collected packages: deeplift\n",
      "  Building wheel for deeplift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deeplift: filename=deeplift-0.6.13.0-py3-none-any.whl size=36447 sha256=1f95bc026012ba8839cc2255a87f25b51f83322ce14127b129e086efd9079e88\n",
      "  Stored in directory: /home/yiyou/.cache/pip/wheels/80/42/80/d6af8dbe1e394d4696459ed54b21787722b9bcb9e240dd81f5\n",
      "Successfully built deeplift\n",
      "Installing collected packages: deeplift\n",
      "Successfully installed deeplift-0.6.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deeplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fec6680-ed75-47b2-a286-76de71a49440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta2binonehot(data):\n",
    "    # data is a list of sequence: [n,seqlength]\n",
    "    # possibly need list version where seqlength differ\n",
    "    data=np.squeeze(np.array(list(map(list, data))))\n",
    "    A = np.zeros_like(data,dtype=int)\n",
    "    C = np.zeros_like(data,dtype=int)\n",
    "    G = np.zeros_like(data,dtype=int)\n",
    "    U = np.zeros_like(data,dtype=int)\n",
    "    A[data == 'A'] = 1\n",
    "    C[data == 'C'] = 1\n",
    "    G[data == 'G'] = 1\n",
    "    U[data == 'U'] = 1\n",
    "    U[data == 'T'] = 1\n",
    "    A = A[..., np.newaxis]\n",
    "    C = C[..., np.newaxis]\n",
    "    G = G[..., np.newaxis]\n",
    "    U = U[..., np.newaxis]\n",
    "    bindata=np.append(A,C,axis=-1)\n",
    "    bindata = np.append(bindata, G, axis=-1)\n",
    "    bindata = np.append(bindata, U, axis=-1)\n",
    "    return bindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ceffc9-0db2-4e54-b31d-0e3d8966c67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252009, 2001, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_list=[]\n",
    "for seq_record in SeqIO.parse('%s/sequence.fasta'%(header),format='fasta'):\n",
    "    sequence=seq_record.seq\n",
    "    seq_list.append(sequence)\n",
    "seq_list=np.asarray(seq_list)\n",
    "sequence=fasta2binonehot(seq_list)\n",
    "sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a74938-fc79-4b0b-9f6a-afa4a1d94200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAdaptPooling(nn.Module):\n",
    "    def __init__(self, model, outsizelist=np.array([9, 25, 64])):\n",
    "        super(MultiAdaptPooling, self).__init__()\n",
    "        self.model = model\n",
    "        self.modellist = []\n",
    "        for i in outsizelist:\n",
    "            self.modellist.append(nn.AdaptiveAvgPool1d(i))\n",
    "    def forward(self, x):\n",
    "        outlist = []\n",
    "        for model in self.modellist:\n",
    "            outlist.append(self.model(model(x)))\n",
    "        out=torch.cat(outlist, -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "74d300d2-a462-4baa-96a1-f2c57810e904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExpressRM(\n",
       "  (conv_model): Sequential(\n",
       "    (0): Conv1d(4, 64, kernel_size=(7,), stride=(5,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "    (4): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): Dropout(p=0.25, inplace=False)\n",
       "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
       "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.01)\n",
       "    (12): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (adaptconv_model): MultiAdaptPooling(\n",
       "    (model): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.25, inplace=False)\n",
       "      (4): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
       "      (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "      (7): Dropout(p=0.25, inplace=False)\n",
       "      (8): AdaptiveAvgPool1d(output_size=21)\n",
       "      (9): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
       "      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): LeakyReLU(negative_slope=0.01)\n",
       "      (12): Dropout(p=0.25, inplace=False)\n",
       "      (13): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
       "      (14): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (15): LeakyReLU(negative_slope=0.01)\n",
       "      (16): Dropout(p=0.25, inplace=False)\n",
       "      (17): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (geneenc): Sequential(\n",
       "    (0): Linear(in_features=28278, out_features=1000, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (predicationhead): Sequential(\n",
       "    (0): Linear(in_features=2817, out_features=2048, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.25, inplace=False)\n",
       "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.25, inplace=False)\n",
       "    (9): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExpressRM(pl.LightningModule):\n",
    "    # unet assume seqlength to be ~500\n",
    "    def __init__(self,useseq=True,usegeo=True,usetgeo=True,usegene=True,usegenelocexp=True, patchsize=7, patchstride=5, inchan=4, dim=64, kernelsize=7,\n",
    "                 adaptoutsize=9, geneoutsize=500, geooutsize=32, droprate=0.25, lr=2e-5):\n",
    "        super(ExpressRM, self).__init__()\n",
    "        self.useseq = useseq\n",
    "        self.usegeo = usegeo\n",
    "        self.usegene = usegene\n",
    "        self.usegenelocexp = usegenelocexp\n",
    "        self.usetgeo = usetgeo\n",
    "        self.droprate = droprate\n",
    "        self.seqoutsize = 4 * adaptoutsize * dim\n",
    "        self.geneoutsize = geneoutsize\n",
    "        self.geooutsize = geooutsize\n",
    "        self.learning_rate = lr\n",
    "        self.posweight=torch.as_tensor(3.0)\n",
    "        self.save_hyperparameters()\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=inchan, out_channels=dim, kernel_size=patchsize, stride=patchstride),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(droprate),\n",
    "            nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=kernelsize),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(droprate),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=kernelsize),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(droprate))\n",
    "        self.adaptconv_model = MultiAdaptPooling(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=kernelsize),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(droprate),\n",
    "                nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=kernelsize),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(droprate),\n",
    "                nn.AdaptiveAvgPool1d(adaptoutsize + 2*(kernelsize - 1)),\n",
    "                nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=kernelsize),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(droprate),\n",
    "                nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=kernelsize),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(droprate),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            , np.array([16, 32, 64, 128]))\n",
    "        self.geneenc = nn.Sequential(nn.Linear(28278, 1000), nn.LeakyReLU(), nn.Dropout(self.droprate),\n",
    "                                     nn.Linear(1000, self.geneoutsize), nn.LeakyReLU())\n",
    "        self.predicationhead = nn.Sequential(\n",
    "            # nn.Flatten(1,-1),\n",
    "            nn.Linear(self.seqoutsize + self.geneoutsize + 12 + 1, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(droprate),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(droprate),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(droprate),\n",
    "            nn.Linear(1024, 4),\n",
    "        )\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    def forward(self, x,geo,gene,genelocexp):\n",
    "        # x=mergedinput[:,:8004].reshape([-1,2001,4])\n",
    "        # geo=mergedinput[:,8004:8016].reshape([-1,1,12])\n",
    "        # gene=mergedinput[:,8016:8016+28278].reshape([-1,28278])\n",
    "        # genelocexp=mergedinput[:,-1].reshape([-1,1,1])\n",
    "        # print(x.shape)\n",
    "        # print(geo.shape)\n",
    "        # print(gene.shape)\n",
    "        # print(genelocexp.shape)\n",
    "        \n",
    "        batchsize = x.size()[0]\n",
    "        tissuesize = 1\n",
    "        if self.useseq:\n",
    "            x = x.transpose(-1, -2)\n",
    "            adaptout = self.adaptconv_model(self.conv_model(x))\n",
    "        # seq [N,2304]\n",
    "        if self.usegene:\n",
    "            # gene= self.geneenc(torch.mean(self.geneatt(geneloc,gene),dim=-2))\n",
    "            gene= self.geneenc(gene)\n",
    "        else:\n",
    "            gene= torch.zeros([batchsize,tissuesize,self.geneoutsize]).float().cuda()\n",
    "            #[N,37,24]\n",
    "        if not self.usetgeo:\n",
    "                    geo[:,:,6:]*=0\n",
    "        if not self.usegeo:\n",
    "                geo[:, :, :6] *= 0\n",
    "        if not self.usegenelocexp:\n",
    "            genelocexp*=0\n",
    "        # for entry in [adaptout, gene, geo.squeeze(1), genelocexp]:\n",
    "        #     print(entry.shape)\n",
    "        adaptout = torch.cat([adaptout, gene, geo.squeeze(1), genelocexp], dim=-1)\n",
    "        out = self.predicationhead(adaptout)\n",
    "        return out\n",
    "device='cpu'\n",
    "model=ExpressRM().load_from_checkpoint('/home/yiyou/test/model.ckpt',map_location=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02bd829e-d79e-4c42-a90e-247677544b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "refgeo=np.asarray(pd.read_csv('%s/geo.csv'%(header)))[:,6:]\n",
    "tgeo=np.asarray(pd.read_csv('%s/tgeo0.csv'%(header)))[:,6:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "38c3bab1-a528-408f-b740-a4fade605ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252009, 38, 12)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_list=[]\n",
    "geo=np.append(refgeo,tgeo,axis=-1)\n",
    "geo_list.append(geo)\n",
    "for tissue in np.array(generalexppd.columns):\n",
    "    tgeo=np.asarray(pd.read_csv('/home/yiyou/tissue/geo/%s.csv'%(tissue)))[:,6:]\n",
    "    geo=np.append(refgeo,tgeo,axis=-1)\n",
    "    geo_list.append(geo)\n",
    "geo=np.array(geo_list).transpose([1,0,2])\n",
    "geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5599cdbd-a707-423c-897c-39f71533d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostgeneexp=np.asarray(pd.read_csv('%s/lg2hosting_expression.csv'%(header)))[:,:1]\n",
    "geneexp=np.asarray(pd.read_csv('%s/lg2geneexp.csv'%(header),index_col=0))[:,:1]\n",
    "# generalhostexp=np.asarray(pd.read_csv('/home/yiyou/lg2hosting_expression.csv',index_col=0))\n",
    "generalexp=np.asarray(pd.read_csv('/home/yiyou/lg2geneexp.csv',index_col=0))\n",
    "generalexppd=pd.read_csv('/home/yiyou/lg2geneexp.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "496162b9-4a27-4926-b4a3-ef4fc4b6837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geneexp=np.append(geneexp,generalexp,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ec2c94c8-58ff-480f-ae89-63cb0627e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence=torch.as_tensor(sequence).float().to(device)\n",
    "geo=torch.as_tensor(geo).float().to(device)\n",
    "hostgeneexp=torch.as_tensor(hostgeneexp).float().unsqueeze(2).to(device)\n",
    "geneexp=torch.as_tensor(geneexp).float().transpose(1,0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "418948ef-ab47-45e4-b268-b6d0916162df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entry(i,j):\n",
    "    return [sequence[i:i+1], geo[i:i+1,j], geneexp[j:j+1], hostgeneexp[i:(i+1),0]]\n",
    "def extract_group_entry(npi,npj):\n",
    "    entry_list=[[],[],[],[]]\n",
    "    for j in npj:\n",
    "        for i in npi:\n",
    "            entry=extract_entry(i,j)\n",
    "            for k in range(4):\n",
    "                entry_list[k].append(entry[k])\n",
    "    \n",
    "    for k in range(4):\n",
    "        print(k)\n",
    "        entry_list[k]=torch.cat(entry_list[k])\n",
    "    return entry_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c8635617-af2c-43c3-82d5-8c042aea4629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "torch.Size([1900, 2001, 4])\n",
      "torch.Size([1900, 12])\n",
      "torch.Size([1900, 28278])\n",
      "torch.Size([1900, 1])\n",
      "torch.Size([1900, 2304])\n",
      "torch.Size([1900, 500])\n",
      "torch.Size([1900, 12])\n",
      "torch.Size([1900, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2001, 4])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 28278])\n",
      "torch.Size([50, 1])\n",
      "torch.Size([50, 2304])\n",
      "torch.Size([50, 500])\n",
      "torch.Size([50, 12])\n",
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "shap_explainer = shap.GradientExplainer(model, extract_group_entry(np.arange(50),np.arange(38)))\n",
    "raw_shap_explanations = shap_explainer.shap_values(extract_entry(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fbdfba6b-4e1b-45b7-ab68-16f7e59daa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28278)\n",
      "-0.45865870984287443\n",
      "-0.17472765757842137\n",
      "-0.02175326724908\n",
      "-0.0034240561220748294\n",
      "0.0023451372628989792\n",
      "0.025745516929735333\n",
      "0.07694365741983478\n",
      "0.32651892193666743\n",
      "0.6129571111891621\n"
     ]
    }
   ],
   "source": [
    "def quantiles(a):\n",
    "    print(a.shape)\n",
    "    print(np.quantile(a,0.001))\n",
    "    print(np.quantile(a,0.01))\n",
    "    print(np.quantile(a,0.1))\n",
    "    print(np.quantile(a,0.25))\n",
    "    print(np.quantile(a,0.5))\n",
    "    print(np.quantile(a,0.75))\n",
    "    print(np.quantile(a,0.9))\n",
    "    print(np.quantile(a,0.99))\n",
    "    print(np.quantile(a,0.999))\n",
    "quantiles(raw_shap_explanations[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "323ddb98-c3ae-4776-aa6d-6fd91650fc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HYAL2', 'CTSC', 'AKAP13', 'HIVEP2', 'THBS1', 'PPP4R3A', 'FBXL12',\n",
       "       'MYL12B', 'ARHGAP21', 'ATN1', 'HSPA2', 'RPPH1', 'SOS1', 'MIER3',\n",
       "       'PCDHGA10', 'ZC3H4', 'DIPK1B', 'IER5L', 'DYRK1A', 'ZMIZ1'],\n",
       "      dtype='object', name='GeneName')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/yiyou/lg2geneexp.csv',index_col=0).index[np.argsort(raw_shap_explanations[0][2][0])[-20:]][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e313aa44-c65c-45a1-aecf-60331a71d0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['JRKL', 'AGPAT2', 'MARCHF2', 'ETS2', 'PARP9', 'DNAJC3-DT', 'KPNA1',\n",
       "       'MICALL2', 'ASH1L-AS1', 'DPH7', 'RBM12', 'RNA18SN5', 'TASOR2', 'KLF10',\n",
       "       'THAP5', 'POMGNT2', 'CCDC174', 'PIP5K1C', 'ATOX1', 'HERPUD1'],\n",
       "      dtype='object', name='GeneName')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/yiyou/lg2geneexp.csv',index_col=0).index[np.argsort(raw_shap_explanations[0][2][0])[:20]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
